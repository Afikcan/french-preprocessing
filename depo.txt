question = "Quelle sont les exemple pour l'apprentissage automatique ?"
context = "Pour nombre d'entre nous, l'apprentissage automatique semble assez futuriste. Pourtant, depuis quelque temps, on le retrouve de plus en plus dans notre quotidien, que ce soit sous la forme d'un ordinateur Google livrant une partie de go palpitante ou de la création de réponses automatiques dans Inbox by Gmail."


nlp = pipeline('question-answering', model='fmikaelian/camembert-base-fquad', tokenizer='fmikaelian/camembert-base-fquad')
restul_fmikaelian = nlp({
    'question': question,
    'context': context
})

nlp_etalab = pipeline('question-answering', model='etalab-ia/camembert-base-squadFR-fquad-piaf', tokenizer='etalab-ia/camembert-base-squadFR-fquad-piaf')
result_etalabia = nlp_etalab({
    'question': question,
    'context': context
})

print('restul_fmikaelian:')
print(restul_fmikaelian)
print('result_etalabia:')
print(result_etalabia)

---------

french_stopwords = nltk.corpus.stopwords.words('french')
mots = set(line.strip() for line in open('NLP_Preprocessing/dictionnaire.txt', encoding="utf8"))

lemmatizer = FrenchLefffLemmatizer()
stemmer = FrenchStemmer()


def French_Preprocess_listofText(listofText):
    preprocess_list = []
    for text in listofText:
        # deleting the apostrophes in words. For exemple: l'apprentisage
        print(type(text))
        sentence_w_apostrophe = text.replace('\'', ' ')

        print(type(sentence_w_apostrophe))
        # deleting all punctuations
        sentence_w_punct = "".join([i.lower() for i in sentence_w_apostrophe if i not in string.punctuation])

        print(type(sentence_w_punct))
        # deleting all numbers
        sentence_w_num = ''.join(i for i in sentence_w_punct if not i.isdigit())

        print(type(sentence_w_num))
        # spliting words
        tokenize_sentence = nltk.tokenize.word_tokenize(sentence_w_num)

        print(type(tokenize_sentence))
        # deleting stopwords
        words_w_stopwords = [i for i in tokenize_sentence if i not in french_stopwords]

        print(type(words_w_stopwords))
        # stemming
        french_stem = [stemmer.stem(word) for word in words_w_stopwords]

        # lemmatizing
        # words_lemmatize = (lemmatizer.lemmatize(w) for w in words_w_stopwords)
        # sentence_clean = ' '.join(w for w in words_lemmatize if w.lower() in mots or not w.isalpha())

        print(type(french_stem))
        # deleting not alphabet letters that are not punctuations. For exemple: £, ¨
        sentence_clean = ' '.join(w for w in french_stem if w != w.isalpha())

        print(type(sentence_clean))
        preprocess_list.append(sentence_clean)

    return preprocess_list


6
lst = ['C\'est un test pour lemmatizer',
       'plusieurs phrases pour un nettoyage',
       'eh voilà la troisième !',
       'il y\'a de cela 2 semaines']

lst_2 = [
    "Nous avons donc demandé à Maya Gupta, chercheuse chez Google dans le domaine de l'apprentissage automatique, "
    "de nous expliquer tout cela.",
    "Pour nombre d'entre nous, l'apprentissage automatique semble assez futuriste. Pourtant, depuis quelque temps, "
    "on le retrouve de plus en plus dans notre quotidien, que ce soit sous la forme d'un ordinateur Google livrant "
    "une partie de go palpitante ou de la création de réponses automatiques dans Inbox by Gmail.",
    ]

index = 3

french_text = pd.DataFrame(lst, columns=['text'])

french_preprocess_list = French_Preprocess_listofText(french_text['text'])

print('Base sentence : ' + lst[index])
print('Cleaned sentence : ' + french_preprocess_list[index])

